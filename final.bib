
@article{kang_ddcolor_2022,
	title = {{DDColor}: {Towards} {Photo}-{Realistic} {Image} {Colorization} via {Dual} {Decoders}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{DDColor}},
	url = {https://arxiv.org/abs/2212.11613},
	doi = {10.48550/ARXIV.2212.11613},
	abstract = {Image colorization is a challenging problem due to multi-modal uncertainty and high ill-posedness. Directly training a deep neural network usually leads to incorrect semantic colors and low color richness. While transformer-based methods can deliver better results, they often rely on manually designed priors, suffer from poor generalization ability, and introduce color bleeding effects. To address these issues, we propose DDColor, an end-to-end method with dual decoders for image colorization. Our approach includes a pixel decoder and a query-based color decoder. The former restores the spatial resolution of the image, while the latter utilizes rich visual features to refine color queries, thus avoiding hand-crafted priors. Our two decoders work together to establish correlations between color and multi-scale semantic representations via cross-attention, significantly alleviating the color bleeding effect. Additionally, a simple yet effective colorfulness loss is introduced to enhance the color richness. Extensive experiments demonstrate that DDColor achieves superior performance to existing state-of-the-art works both quantitatively and qualitatively. The codes and models are publicly available at https://github.com/piddnad/DDColor.},
	urldate = {2024-05-14},
	author = {Kang, Xiaoyang and Yang, Tao and Ouyang, Wenqi and Ren, Peiran and Li, Lingzhi and Xie, Xuansong},
	year = {2022},
	note = {Publisher: [object Object]
Version Number: 5},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	annote = {Other
ICCV 2023; Code: https://github.com/piddnad/DDColor},
}

@article{levin_colorization_2004,
	title = {Colorization using optimization},
	volume = {23},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/1015706.1015780},
	doi = {10.1145/1015706.1015780},
	abstract = {Colorization is a computer-assisted process of adding color to a monochrome image or movie. The process typically involves segmenting images into regions and tracking these regions across image sequences. Neither of these tasks can be performed reliably in practice; consequently, colorization requires considerable user intervention and remains a tedious, time-consuming, and expensive task.In this paper we present a simple colorization method that requires neither precise image segmentation, nor accurate region tracking. Our method is based on a simple premise; neighboring pixels in space-time that have similar intensities should have similar colors. We formalize this premise using a quadratic cost function and obtain an optimization problem that can be solved efficiently using standard techniques. In our approach an artist only needs to annotate the image with a few color scribbles, and the indicated colors are automatically propagated in both space and time to produce a fully colorized image or sequence. We demonstrate that high quality colorizations of stills and movie clips may be obtained from a relatively modest amount of user input.},
	language = {en},
	number = {3},
	urldate = {2024-05-14},
	journal = {ACM Transactions on Graphics},
	author = {Levin, Anat and Lischinski, Dani and Weiss, Yair},
	month = aug,
	year = {2004},
	pages = {689--694},
}

@article{welsh_transferring_2002,
	title = {Transferring color to greyscale images},
	volume = {21},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/566654.566576},
	doi = {10.1145/566654.566576},
	abstract = {We introduce a general technique for "colorizing" greyscale images by transferring color between a source, color image and a destination, greyscale image. Although the general problem of adding chromatic values to a greyscale image has no exact, objective solution, the current approach attempts to provide a method to help minimize the amount of human labor required for this task. Rather than choosing RGB colors from a palette to color individual components, we transfer the entire color "mood" of the source to the target image by matching luminance and texture information between the images. We choose to transfer only chromatic information and retain the original luminance values of the target image. Further, the procedure is enhanced by allowing the user to match areas of the two images with rectangular swatches. We show that this simple technique can be successfully applied to a variety of images and video, provided that texture and luminance are sufficiently distinct. The images generated demonstrate the potential and utility of our technique in a diverse set of application domains.},
	language = {en},
	number = {3},
	urldate = {2024-05-14},
	journal = {ACM Transactions on Graphics},
	author = {Welsh, Tomihisa and Ashikhmin, Michael and Mueller, Klaus},
	month = jul,
	year = {2002},
	pages = {277--280},
}

@inproceedings{yu-wing_tai_local_2005,
	address = {San Diego, CA, USA},
	title = {Local {Color} {Transfer} via {Probabilistic} {Segmentation} by {Expectation}-{Maximization}},
	volume = {1},
	isbn = {978-0-7695-2372-9},
	url = {http://ieeexplore.ieee.org/document/1467343/},
	doi = {10.1109/CVPR.2005.215},
	urldate = {2024-05-14},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	publisher = {IEEE},
	author = {{Yu-Wing Tai} and {Jiaya Jia} and {Chi-Keung Tang}},
	year = {2005},
	pages = {747--754},
	file = {Submitted Version:/Users/zack4/Zotero/storage/XRJH54M4/Yu-Wing Tai et al. - 2005 - Local Color Transfer via Probabilistic Segmentatio.pdf:application/pdf},
}

@misc{zhang_colorful_2016,
	title = {Colorful {Image} {Colorization}},
	url = {http://arxiv.org/abs/1603.08511},
	abstract = {Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a "colorization Turing test," asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32\% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
	month = oct,
	year = {2016},
	note = {arXiv:1603.08511 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/zack4/Zotero/storage/3CX6KF3K/Zhang et al. - 2016 - Colorful Image Colorization.pdf:application/pdf;arXiv.org Snapshot:/Users/zack4/Zotero/storage/PI4252BF/1603.html:text/html},
}

@misc{royer_probabilistic_2017,
	title = {Probabilistic {Image} {Colorization}},
	url = {http://arxiv.org/abs/1705.04258},
	abstract = {We develop a probabilistic technique for colorizing grayscale natural images. In light of the intrinsic uncertainty of this task, the proposed probabilistic framework has numerous desirable properties. In particular, our model is able to produce multiple plausible and vivid colorizations for a given grayscale image and is one of the first colorization models to provide a proper stochastic sampling scheme. Moreover, our training procedure is supported by a rigorous theoretical framework that does not require any ad hoc heuristics and allows for efficient modeling and learning of the joint pixel color distribution. We demonstrate strong quantitative and qualitative experimental results on the CIFAR-10 dataset and the challenging ILSVRC 2012 dataset.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Royer, Amelie and Kolesnikov, Alexander and Lampert, Christoph H.},
	month = may,
	year = {2017},
	note = {arXiv:1705.04258 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/zack4/Zotero/storage/GWSF4353/Royer et al. - 2017 - Probabilistic Image Colorization.pdf:application/pdf;arXiv.org Snapshot:/Users/zack4/Zotero/storage/F7MCPEVK/1705.html:text/html},
}
